% This is LLNCS.DEM the demonstration file of
% the LaTeX macro package from Springer-Verlag
\documentclass[a4paper,12pt]{llncs}
%
\usepackage{makeidx}  % allows for indexgeneration
\makeindex

\usepackage[ngerman]{babel}
\usepackage[utf8]{inputenc}      % Code-Page latin 1
\usepackage[T1]{fontenc}
\usepackage{listings}
% Nur eine der beiden folgenden Zeilen einbinden!
% siehe Abschnitt Bilder
%\usepackage{graphicx}       % Bilder einbinden, Version fuer normales latex
\usepackage[pdftex]{graphicx}       % Bilder einbinden, Version fuer pdflatex

% mit Hyperrefs
\usepackage[pdftex, plainpages=false,hypertexnames=true,pdfnewwindow=true,backref=true,colorlinks=true,citecolor=blue,linkcolor=black,urlcolor=blue,filecolor=blue]{hyperref}%
% weitere Packages
\usepackage{ifthen}                 % Zum Auskommentieren von Textteilen
\usepackage{amssymb}                % Mathematische Buchstaben
\usepackage{amsmath}                % Verbesserter Formelsatz
\usepackage{booktabs}               % schönere Tabellen
\usepackage{color}
\usepackage{hyperref}
 \hypersetup{urlcolor=black,citecolor=black}
\usepackage{dsfont}
%\newtheorem{definition}{Definition}
\usepackage{doc}

% Seitenformat ===============================================================
\hoffset=-1.25truecm
\setlength{\topmargin}{0.0cm}
\setlength{\textheight}{23.0cm}
\setlength{\footskip}{1.5cm}
\setlength{\textwidth}{15.4cm}
\setlength{\evensidemargin}{1.5cm}
\setlength{\oddsidemargin}{1.5cm}
\setlength{\parskip}{1ex}
\setlength{\parindent}{0pt}
\setlength{\marginparwidth}{1.4cm}
\setlength{\marginparsep}{1mm}

\pagestyle{plain}

% LstListing-Format ==========================================================
\lstdefinestyle{cpp}{
  language=C++,
  basicstyle=\small\ttfamily,
  frame=tb,
  xleftmargin=\parindent,
  keywordstyle=\color{blue},
  stringstyle=\color{red},
  commentstyle=\color{green},
  morecomment=[l][\color{magenta}]{\#},
  framexleftmargin=5pt,
  framexrightmargin=5pt,
  framextopmargin=5pt,
  framexbottommargin=5pt,
  literate={~}{$\sim$}1
}

% Makro-Definitionen ==========================================================
% Zahlenbereiche -------------------------------------------------------------
\newcommand{\N}{{\mathbb{N}}}
\newcommand{\R}{{\mathbb{R}}}
\newcommand{\C}{{\mathbb{C}}}
\newcommand{\Z}{{\mathbb{Z}}}
\newcommand{\Q}{{\mathbb{Q}}}

%
\def\myverzeichnis{.}

\numberwithin{equation}{section}
% Bild -----------------------------------------------------------------------
% #1 Filename;  #2 Label;  #3 Bildunterschrift;  #4 Kurzform
\newcommand{\bild}[4]{
  \begin{figure}[htbp]
    \begin{center}
      \includegraphics{#1}
      \caption[#4]{#3}
      \label{#2}
    \end{center}
  \end{figure}
}

% Bildbreite -----------------------------------------------------------------
% #1 Filename;  #2 Breite;  #3 Label;  #4 Bildunterschrift;  #5 Kurzform
\newcommand{\bildbreite}[5]{
  \begin{figure}[htbp]
    \begin{center}
      \includegraphics[width=#2]{#1}
      \caption[#5]{#4}
      \label{#3}
    \end{center}
  \end{figure}
}


% ============================================================================
\begin{document}

% =========== Das war der Vorspann, jetzt geht's los! ========================

% ============================================================================
% =============  AB HIER DARF UND SOLL GETIPPT WERDEN ========================
% ============================================================================

\author{Tobias Schiffmann}
\index{Viel Schreiber}

% Das Institut wird fuer den Betreuer missbraucht ...
\institute{{\bf Betreuer:} Gregor Daiß}
\authorrunning{Viel Schreiber}
\title{SIMT/GPGPU - CUDA \& OpenCL}

\maketitle

\thispagestyle{empty}

\begin{abstract}
Ein schöner Abstract. Das ist einfach die Kurzzusammenfassung.
\end{abstract}


\section{Motivation}
  1 entworfen für Grafikanwendungen mit sehr großen Datenmengen\\
    --> haben sich relativ unabhängig von CPUs entwickelt\\
    --> im Grafikbereich: hohes Potential an Datenparallelität! --> keine Abhängigkeiten\\
  1 sehr hoher FP-Operation durchsatz, verteilt auf große Anzahl Threads\\
  4 interessant in anderen Anwendungsgebieten --> General Purpose GPU\\
    
%---------------------------------------------------------------------------------
    
\section{Difference between CPU and GPU}

  9 GPU is specialized for highly parallel computation --> graphics rendering works
  9 more transistors are devoted to data processing than caching and flow control
  \bildbreite{figures/GPU_CPU_Flops.JPG}{12cm}{fig:GPU_CPU}{9 CPU and GPU FLOPS Development)}{}
    \bildbreite{figures/CPU_GPU_Arc.JPG}{9cm}{fig:GPU_CPU_Arc}{9 evtl CPU and GPU Architecture)}{}

    2 parallelism can even go up to task parallelism --> SIMT
    2 intensive floating point arithmetic
  
  --> show a picture of CPU architecture and the amount of cores used
  
\subsection{GPU Architecture}
    - Function units / architecture picture\\
      1 multi-threaded SIMD-Prozessoren (NVIDEA: Streaming Multiprocessors [SMX]), können als unabhängige MIMD-Kerne betrachtet werden\\
      1 jeder SIMD Prozessor hat mehrere SIMD-Funktionseinheiten( jede hat Int und FP - Einheit)\\
	  3 Turing architecture\\
	  - Figure \ref{fig:turingOverall} mit Turing architecture\\
	  \bildbreite{figures/Turing_architecture.JPG}{6cm}{fig:turingOverall}{8 Turing Architecture TU106 (RTX2070)}{}
	  - Figure \ref{fig:smArch} SM Architecture\\
	  \bildbreite{figures/SM_arch.jpg}{4cm}{fig:smArch}{8 SM Architecture}{}
	  8 2 SMs per TPC (Texture Processing Cluster)\\
	  8 SM is divided into 4 processing blocks\\
	    - each with 16 FP32 Cores, 16 INT32 Cores, two Tensor Cores, one warp scheduler, and one dispatch unit\\
	    - each block includes a new L0 instruction cache and a 64 KB register file\\
	    - all share a combined 96 KB L1 data cache/shared memory.\\

	  3 Tensor Core as special Deep Learning Accelerator\\
	    - Matrix-Accumulate functions --> essential part in deep learning\\
	  3 RTCore accelerates ray tracing\\
	    - Accelerate ray probing and move it to other function units\\
	    - SM can do other tasks meanwhile\\
	  
	  
    - memory hierarchy\\
      1 + 4 verschiedene Speichertypen\\
        - global memory(host-RW, Device-RW)\\
        - constant memory (host-RW, device-R)\\
        - register (local - 2)(nur thread kann drauf zu greifen) and shared memory(alle threads eines Blocks) (kurze Zugriffszeit)\\
        - Figure \ref{fig:memorga}
    \bild{figures/speicheroragnisation.jpg}{fig:memorga}{1 Memory Organization}{}
        
\subsection{Latency Hiding}
  1 verbergen der Speicherzugriffe durch umschalten zwischen mehrereren Threads
  9 GPU hides memory access latencies with computation instead of avoiding memory access latencies through large data caches and flow control


\subsection{Thread Scheduling}
    1 passiert nicht auf der Ebene einzelner Threads, sondern "Warps" --> mehrere Threads eines Blocks (aufsteigende threadIDx) zusammengefasst\\
    2 threads of a warp start at the same program address, but may diverge\\
    1 + 2 eine Instruktion nach der anderen für alle aktiven Threads eines Warps ausführen --> \textbf{SIMT}\\
    2 --> individual threads can be inactive due to independent branching\\
    1+2 --> Instruktionen auf verschiedenen Kontrollflüssen, wenn gleich, dann kann schnell sein\\
      1 Falls, unterschiedlich SIMT nur beschränkt möglich, kann sich auch bei if-else aufteilen\\
      2 in case of diverge, serially execute each branch path --> reconverge is possible --> use synchronization stack to manage independent diverging and converging threads\\
      2 SIMD instruction controls vector of multiple data lanes together and exposes the vector width to the software, whereas a SIMT instruction controls the execution and branching behavior of one thread\\
      2 SIMT is only necessary to consider when programming if peak performance wants to be achieved --> SIMD needs to be explicitly declared\\

    2 instruction scheduler prioritizes all ready warps in a scoreboard and selects the one with highest priority\\
    2 the score considers warp type, instruction type and "fairness" to all warps\\

    2 in a thread block, fast barrier synchronization instructions to wait for writes to shared or global memory to complete before reading data\\
    2 sequentially dependent grids use a global intergrid synchronization barrier between grids to ensure global read/write ordering\\
   
(evtl. 2 thread types (vertex, geometry, pixel and compute)

         
\subsection{Performance}
  - Examples for each in Peak Performance GFLOPS\\
    --> Intel Xeon Platinum 8276 Processor \\
    --> white papers of the products --> 8
\begin{table}[htbp]
  \centering
  \caption{Metric Comparison Intel Cascade Lake and NVIDIA Turing TU106}
  \label{tab:comp}
  \begin{tabular}{|l|c|c|c|}
    \hline
	\textbf{Metric} & \textbf{~Intel (Cascade Lake)~} & \textbf{~Tesla P4 (Pascal)~} & \textbf{~Tesla T4 (Turing)~} \\\hline
	Physical Cores & 28 & 2560 & 2560 \\\hline
	Max Frequency in MHz & 4000 & 1063 & 1590 \\\hline
	Peak FLOPS in Giga-FLOPS & & 5500 & 8100 \\\hline
  \end{tabular}
\end{table}

%---------------------------------------------------------------------------------

\section{CUDA - A GPU Programming Model}
      1 NVIDIA as vendor\\
      9 intention is to maintain a low learning curve for programmers
      9 three core abstractions - hierarchy of thread groups, shared memories and barrier synchronization
      1+9 program separation in host program (CPU [IO + User]) and device program (GPU)\\
      1+9 device program = kernel-functions/kernels\\
	  1 interaction: host program copies data in GPU memory and calls device function\\
	  1+9 a program starts via a host program until kernel function is called -> then both run in parallel till synchronization\\
		- Call creates CUDA- Threads (summarized as Grid)\\
	  1+9 CUDA has different declaration specifiers\\
        - \_\_global\_\_ --> declare as kernel function\\
        - \_\_device\_\_ --> kernel function which can only be called by other kernel functions\\
        - \_\_host\_\_ --> host function (default)\\
	  1+4 kernel call contains \textit{execution configuration}, <<<Dg, Db>>>\\
   	    - Dg: type dim3, specifies the dimension and size of the grid\\
   	    - Db: type dim3, specifies the dimension and size of each block\\
	    --> specifies the organization of the generated Threads in the Grid of the kernel function\\
	  	  - unterteile Threads des Grids in Blöcke von Threads (3-dim, blockIDx.xyz)\\
	  	  - Blöcke sind in threads aufgeteilt (3-dim, threadIDx.xyz)\\
	   --> jeder einzelne Thread des Grids kann aufgerufen werden eine Kernel-Funktion auszuführen\\
          - um Threads zu synchronisieren gibt es synchronisationsfunktionen nur innerhalb eines Blocks (\_\_syncthreads())\\
          - deshalb müssen blöcke unabhängig voneinander ausführbar sein\\
  
    9 device memory is allocated and deallocated by cudaMalloc() and cudaFree(), it is moved between device and host via cudaMemcpy()\\
    

  
    2 GPU computing architecture transparently scales parallel application performance with the number of SMs and SP cores\\
    9+2 executes on any size of GPU without recompiling --> program does not care how many processors it uses\\
      --> problem is decomposed into independently computed blocks\\
      --> GPU compute work distribution unit generates a stream of thread blocks and distributes them to available SMs\\
     9+2 each kernel call dynamically creates a new grid with the right number of blocks and threads for that application\\



     EVTL: 9 streams and graphs

  --> Example Implementation: Matrix - Matrix Multiplication\\
    2 nvcc compiler\\

%---------------------------------------------------------------------------------
 
\section{OpenCL - A Multipurpose Programming Model}
  --> Differences to CUDA\\
  - Terminology\\
      1 Work-Items (CUDA-Threads) < Work-Groups < global NDRanges (Grid)\\
  - Basic information\\
      1 multiple partners (including NVIDIA)\\
      1 standardized programming model\\
      1 heterogeneous platform which needs explicit specification (Context)\\
        --> can also be used for FPGAs\\
        --> more complex than CUDA, but provides more diversity in HW\\
      


%---------------------------------------------------------------------------------

\section{Programming Optimization and Challenges}
  - increase Occupancy\\
  - increase latency\\
  - problems / concerns (warp divergence)\\


%---------------------------------------------------------------------------------

\section{Conclusion and Discussion}

      


\begin{enumerate}
\item \cite{Rauber.2012} (1)
\item \cite{Lindholm.2008} (2)
\item \cite{Burgess.2020} (3)
\item \cite{Huang.2008} (4)
\item \cite{Bialas.2016} (5)
\item \cite{Khronos.2019} (6)
\item \cite{Wang.2019} (7)
\item \cite{NVIDIA.2018} (8)
\item \cite{NVIDIA.2019} (9)
\item Intel Xeon 8280L
\end{enumerate}





% Literaturverzeichnis ------------------------------------------------
\newpage
\bibliographystyle{alphadinLinkLocal}
\bibliography{literatur}

%\iffalse
\end{document}
%\fi
