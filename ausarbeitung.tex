% This is LLNCS.DEM the demonstration file of
% the LaTeX macro package from Springer-Verlag
\documentclass[a4paper,12pt]{llncs}
%
\usepackage{makeidx}  % allows for indexgeneration
\makeindex

\usepackage[ngerman]{babel}
\usepackage[utf8]{inputenc}      % Code-Page latin 1
\usepackage[T1]{fontenc}
\usepackage{listings}
% Nur eine der beiden folgenden Zeilen einbinden!
% siehe Abschnitt Bilder
%\usepackage{graphicx}       % Bilder einbinden, Version fuer normales latex
\usepackage[pdftex]{graphicx}       % Bilder einbinden, Version fuer pdflatex

% mit Hyperrefs
\usepackage[pdftex, plainpages=false,hypertexnames=true,pdfnewwindow=true,backref=true,colorlinks=true,citecolor=blue,linkcolor=black,urlcolor=blue,filecolor=blue]{hyperref}%
% weitere Packages
\usepackage{ifthen}                 % Zum Auskommentieren von Textteilen
\usepackage{amssymb}                % Mathematische Buchstaben
\usepackage{amsmath}                % Verbesserter Formelsatz
\usepackage{booktabs}               % schönere Tabellen
\usepackage{color}
\usepackage{hyperref}
 \hypersetup{urlcolor=black,citecolor=black}
\usepackage{dsfont}
%\newtheorem{definition}{Definition}
\usepackage{doc}

% Seitenformat ===============================================================
\hoffset=-1.25truecm
\setlength{\topmargin}{0.0cm}
\setlength{\textheight}{23.0cm}
\setlength{\footskip}{1.5cm}
\setlength{\textwidth}{15.4cm}
\setlength{\evensidemargin}{1.5cm}
\setlength{\oddsidemargin}{1.5cm}
\setlength{\parskip}{1ex}
\setlength{\parindent}{0pt}
\setlength{\marginparwidth}{1.4cm}
\setlength{\marginparsep}{1mm}

\pagestyle{plain}

% LstListing-Format ==========================================================
\lstdefinestyle{cpp}{
  language=C++,
  basicstyle=\small\ttfamily,
  frame=tb,
  xleftmargin=\parindent,
  keywordstyle=\color{blue},
  stringstyle=\color{red},
  commentstyle=\color{green},
  morecomment=[l][\color{magenta}]{\#},
  framexleftmargin=5pt,
  framexrightmargin=5pt,
  framextopmargin=5pt,
  framexbottommargin=5pt,
  literate={~}{$\sim$}1
}

% Makro-Definitionen ==========================================================
% Zahlenbereiche -------------------------------------------------------------
\newcommand{\N}{{\mathbb{N}}}
\newcommand{\R}{{\mathbb{R}}}
\newcommand{\C}{{\mathbb{C}}}
\newcommand{\Z}{{\mathbb{Z}}}
\newcommand{\Q}{{\mathbb{Q}}}

%
\def\myverzeichnis{.}

\numberwithin{equation}{section}
% Bild -----------------------------------------------------------------------
% #1 Filename;  #2 Label;  #3 Bildunterschrift;  #4 Kurzform
\newcommand{\bild}[4]{
  \begin{figure}[htbp]
    \begin{center}
      \includegraphics{#1}
      \caption[#4]{#3}
      \label{#2}
    \end{center}
  \end{figure}
}

% Bildbreite -----------------------------------------------------------------
% #1 Filename;  #2 Breite;  #3 Label;  #4 Bildunterschrift;  #5 Kurzform
\newcommand{\bildbreite}[5]{
  \begin{figure}[htbp]
    \begin{center}
      \includegraphics[width=#2]{#1}
      \caption[#5]{#4}
      \label{#3}
    \end{center}
  \end{figure}
}


% ============================================================================
\begin{document}

% =========== Das war der Vorspann, jetzt geht's los! ========================

% ============================================================================
% =============  AB HIER DARF UND SOLL GETIPPT WERDEN ========================
% ============================================================================

\author{Tobias Schiffmann}
\index{Viel Schreiber}

% Das Institut wird fuer den Betreuer missbraucht ...
\institute{{\bf Betreuer:} Gregor Daiß}
\authorrunning{Viel Schreiber}
\title{SIMT/GPGPU - CUDA \& OpenCL}

\maketitle

\thispagestyle{empty}

\begin{abstract}
Ein schöner Abstract. Das ist einfach die Kurzzusammenfassung.
This \\
Will\\
Fill\\
Up\\
SOme\\
Lines\\
\end{abstract}


\section{Motivation}
  GPUs are increasingly used for a different fields of computing they were created originally.
  Instead of accelerating graphical computations, they are more and more used for e.g. machine learning and mining cryptographic currencies.
  This phenomenon is called \textit{General Purpose} GPU programming.~\cite{8363085}~\cite{Huang.2008}
  
    According to the authors of \cite{NVIDIA.2019} and \cite{Rauber.2012} the GPU is specialized for highly parallel computation.
    It developed rather independent from the CPU development and focused on graphical processing.
    This field provides huge amounts of data to process without internal dependencies.
    Which means, the data can be processed simultaneously.
     
    Figure \ref{fig:GPU_CPU} shows the evolution of CPU and GPU in GFLOPS performance in the last years.
  The GPU has significantly increased its compute power.
  Whereas CPU enhanced rather steadily in comparison.
   \bildbreite{figures/GPU_CPU_Flops.JPG}{14cm}{fig:GPU_CPU}{CPU and GPU FLOPS Development~\cite{NVIDIA.2019}}{}
  
    
%---------------------------------------------------------------------------------
    
\section{Difference between CPU and GPU}

  
  The GPU's high amount of compute power compared to a CPU is achieved by devoting more transistors to data processing than caching and flow control.~\cite{NVIDIA.2019}
  Figure \ref{fig:GPU_CPU_Arc} well illustrates the architectural difference between both in a less detailed way.
  It shows that the amounts of cores (ALU) differ significantly.
  However, the size of cache memory each processing unit can use individually is bigger on in a CPU.
  \bildbreite{figures/CPU_GPU_Arc.JPG}{9cm}{fig:GPU_CPU_Arc}{Differences Between CPU and GPU Architecture~\cite{NVIDIA.2019}}{}
  
  
\subsection{GPU Architecture}
\label{subsec:GPU_Arc}
  A GPU has several multi-threaded SIMD processors.
  In an NVIDIA architecture they are called \textit{Streaming Multiprocessors (SM)}.
  Each of them has multiple processing units for integer and floating point operations. ~\cite{Rauber.2012}

  The newest GPU architecture form NVIDIA is the \textit{Turing} architecture and can be seen in Figure \ref{fig:turingOverall}.
  The example layout \textit{TU106} of a NVIDIA RTX2070 GPU has three Graphic Processing Clusters (GPC), 18 Texture Processing Clusters (TPC) and 36 SMs.~\cite{NVIDIA.2018}
  	  \bildbreite{figures/Turing_architecture.JPG}{\textwidth}{fig:turingOverall}{8 Turing Architecture TU106 (RTX2070)}{}
  	  
  GPCs are self-contained units designed for scalability.
  Creating a new architecture with more processing units can be achieved by adding GPCs.
  For instance the TU102 architecture uses 6 GPCs.
  Each GPC has six TPCs which coordinate the work balancing among their SMs.
  SMs are grouped in pairs and each group is assigned to one TPC.~\cite{Lindholm.2008}~\cite{NVIDIA.2018}
    
  A SM contains four identical blocks.
  Each of them contains 16 integer cores for integer calculations, 16 floating point cores for decimal computation and two Tensor cores to accelerate deep learning.
  It is a special core speeding up a matrix-accumulation function which is an essential part of deep learning model calculation.
  Additionally, each block has its own warp scheduler and dispatch unit for job preparation.
  Inside a block an instruction cache and a register file are placed which are shared among all units inside a block.
  Furthermore, all blocks can access a data cache together which allows data movement across the blocks.
  Figure \ref{fig:smArch} shows the architecture of a SM in detail including all blocks and cores.~\cite{Burgess.2020}~\cite{NVIDIA.2018}
	  \bildbreite{figures/SM_arch.jpg}{6cm}{fig:smArch}{8 SM Architecture}{}

  The figure additionally shows, that a SM has a newly added feature --- the \textit{RTCore}.
  It accelerates ray tracing computation which is used in video game and graphics programming.
  The ray probing task is moved to this function unit and the SM's cores are free to do other tasks in the meanwhile.
  
 Another important thing to mention is memory hierarchy.
 Figure \ref{fig:turingOverall} and \ref{fig:smArch} not only show the processing units inside a GPU.
 They furthermore illustrate the different levels of memory in GPU hardware.
 Different levels of caches can be seen which differ in size and access time.
 The biggest one is the L2 cache on GPC level with 4096 KB.
 All SMs in the GPU can access it which allows communication and data share across them.
 However, this cache is also the slowest when it comes to access time.
 In comparison the L1 cache allows faster data access and it enables data sharing inside a SM.
 Blocks of other SMs cannot access it. 
 It's size is only 96 KB.
 The lowest level of memory are the registers.
 They can only be accessed inside a SM block and have a size of 64 KB.
 However, the registers provide the fastest way to access data.~\cite{Huang.2008}~\cite{NVIDIA.2019}
        


\subsection{Thread Scheduling}
\label{subsec:Thr}
  In contrast to CPUs, GPUs do not schedule threads individually.
  Instead, they are scheduled in groups so called \textit{warps}.
  These groups of threads execute the same program.
  This means they start at the same program address and an instruction is completed by all threads of a warp.
  This type of execution model is called \textit{Single Instruction Multiple Threads (SIMT)}.
  However, a single or multiple may diverge from the other threads due to control flows inside the program, e.g. an if-else statement asking for individual thread IDs.
  In case some threads diverge each control flow is processed by the group of threads following it.
  The threads of the other control flow are inactive and have to wait in the meanwhile.
  This means that the less diverging occurs the faster the program can be completed.
  However, it is important to mention that multiple diverged control flows can converge again into one.
  The schedulers synchronization stack is used to manage independent diverging and converging.~\cite{Rauber.2012}~\cite{Lindholm.2008}

  This is a significant different to Single Instruction Multiple Data (SIMD) which does not allow diverging control flows.
  SIMD instructions have to be explicitly defined by the programmer and they control a vector of fixed length.
  In contrast to that SIMT instructions manage the execution and control path of a thread.
  SIMT does only need to be considered while programming in case performance wants to be increased further.
  
  To schedule a warp of threads the scheduler prioritizes all warps which are ready to be processed in a scoreboard.
  The warp with the highest priority will be scheduled next and processed.
  The priority score considers, e.g. instruction type and "fairness" to all warps.
  There is also the possibility to explicitly synchronize threads by barriers.~\cite{Lindholm.2008}

         
\subsection{Latency Hiding}
  GPUs can use the thread scheduling methods introduced in subsection \ref{subsec:GPU_Arc} to hide the long memory access latency.
  Especially, for the highest level of memory as introduced in subsection \ref{subsec:GPU_Arc}.
  Switching between threads and avoiding stall compute cycles, allows to maximize processor utilization.
  In case a warp wants to access the memory, it calls such a function and goes into idle state.
  The scheduler has the chance now to switch to another ready thread from his scoreboard which will be computed then.
  Therefore, GPUs hide memory access latency by computation instead of caches, e.g. used by CPUs.~\cite{volkov.2016}~\cite{Rauber.2012}~\cite{NVIDIA.2019}
  
  
\subsection{Performance}
  Finally, we will have a look at the actual numbers of performance of GPU and CPU and we will be able to compare them.
  Table \ref{tab:comp} shows the number of physical cores, maximum frequency and peak FLOPS of two GPUs and one CPU.
  The example CPU is an Intel Xeon Platinum 8280 utilizing the Cascade Lake architecture.
  The GPUs are a NVIDIA Tesla P4 of Pascal architecture and a NVIDIA Tesla T4 having the new Turing architecture.
  To calculate the peak performance of a CPU the clock rate has to be multiplied by the number of cores and the number of AVX512 FMA units.
  Then the number of floats fitting into an AVX512 register, which is equal to 16, has to be multiplied by the result.
  Single precision is used as we compare CPUs and GPUs and switching to double precision does not scale as well on GPUs as for CPUs.
  And finally the result has to be multiplied by two, as FMA calculation allows a multiply and an addition per clock.
  \[2.7 * 28 * 2 * 16 * 2 \approx 4838\]
  For this equation the optimistic frequency for all cores utilizing AVX instructions are used --- \(2.7\) GHz.\footnote{www.microway.com/knowledge-center-articles/detailed-specifications-of-the-cascade-lake-sp-intel-xeon-processor-scalable-family-cpus (accessed 09.05.2020)}
  However, this frequency might vary due to thermal issues forcing the CPU to reduce its frequency.
  As the frequency is of scale GHz the result is 4838 GFLOPS.
  The metrics for the GPUs are retrieved from \cite{NVIDIA.2018}.
  
\begin{table}[htbp]
  \centering
  \caption{Metric Comparison Intel Cascade Lake and NVIDIA Turing TU106}
  \label{tab:comp}
  \begin{tabular}{|l|c|c|c|}
    \hline
	\textbf{Metric} & \textbf{~Intel (Cascade Lake)~} & \textbf{~Tesla P4 (Pascal)~} & \textbf{~Tesla T4 (Turing)~} \\\hline
	Physical Cores & 28 & 2560 & 2560 \\\hline
	Max Frequency in MHz & 2700 & 1063 & 1590 \\\hline
	Peak FLOPS in Giga-FLOPS & 4838 & 5500 & 8100 \\\hline
  \end{tabular}
\end{table}


%---------------------------------------------------------------------------------

\section{CUDA - A GPU Programming Model}
\label{sec:CUDA}
  CUDA is developed by NVIDIA to enable an easier way to create programs using GPU acceleration.
  Its goal is to maintain a low learning curve for programmers.
  
  
%      1 NVIDIA as vendor\\
%      9 intention is to maintain a low learning curve for programmers
      
      9 three core abstractions - hierarchy of thread groups, shared memories and barrier synchronization
      1+9 program separation in host program (CPU [IO + User]) and device program (GPU)\\
      1+9 device program = kernel-functions/kernels\\
	  1 interaction: host program copies data in GPU memory and calls device function\\
	  1+9 a program starts via a host program until kernel function is called -> then both run in parallel till synchronization\\
		- Call creates CUDA- Threads (summarized as Grid)\\
	  1+9 CUDA has different declaration specifiers\\
        - \_\_global\_\_ --> declare as kernel function\\
        - \_\_device\_\_ --> kernel function which can only be called by other kernel functions\\
        - \_\_host\_\_ --> host function (default)\\
	  1+4 kernel call contains \textit{execution configuration}, <<<Dg, Db>>>\\
   	    - Dg: type dim3, specifies the dimension and size of the grid\\
   	    - Db: type dim3, specifies the dimension and size of each block\\
	    --> specifies the organization of the generated Threads in the Grid of the kernel function\\
	  	  - unterteile Threads des Grids in Blöcke von Threads (3-dim, blockIDx.xyz)\\
	  	  - Blöcke sind in threads aufgeteilt (3-dim, threadIDx.xyz)\\
	   --> jeder einzelne Thread des Grids kann aufgerufen werden eine Kernel-Funktion auszuführen\\
          - um Threads zu synchronisieren gibt es synchronisationsfunktionen nur innerhalb eines Blocks (\_\_syncthreads())\\
          - deshalb müssen blöcke unabhängig voneinander ausführbar sein\\
  
    9 device memory is allocated and deallocated by cudaMalloc() and cudaFree(), it is moved between device and host via cudaMemcpy()\\
    
\subsection{Memory Hierarchy}
\label{subsec:MemHi}
  Another important aspect to consider when it comes to GPU programming is memory hierarchy.
  A GPU uses different types of memory with different accessibilities and sizes.
  Firstly, there are the global memory and the constant memory.
  They are shared across all SMs in a device.
  However, both differ in accessibility.
  The host, which is the CPU making use of GPU acceleration, can access both via read and write operations to provide data to use.
  In contrast to that the GPU, also known as device, 
  
    
    ~\cite{Rauber.2012}~\cite{Huang.2008}
	  
    - memory hierarchy\\
      1 + 4 verschiedene Speichertypen\\
        - global memory(host-RW, Device-RW)\\
        - constant memory (host-RW, device-R)\\
        - register (local - 2)(nur thread kann drauf zu greifen) and shared memory(alle threads eines Blocks) (kurze Zugriffszeit)\\
     
  Figure \ref{fig:memorga} shows how the memory hierarchy looks like inside of a GPU. 
  \bild{figures/speicheroragnisation.jpg}{fig:memorga}{GPU Memory Organization~\cite{Rauber.2012}}{}
  
\subsection{Hardware Mapping}
  
    2 GPU computing architecture transparently scales parallel application performance with the number of SMs and SP cores\\
    9+2 executes on any size of GPU without recompiling --> program does not care how many processors it uses\\
      --> problem is decomposed into independently computed blocks\\
      --> GPU compute work distribution unit generates a stream of thread blocks and distributes them to available SMs\\
     9+2 each kernel call dynamically creates a new grid with the right number of blocks and threads for that application\\



     EVTL: 9 streams and graphs

  --> Example Implementation: Matrix - Matrix Multiplication\\
    2 nvcc compiler\\

%---------------------------------------------------------------------------------
 
\section{OpenCL - A Multipurpose Programming Model}
  --> Differences to CUDA\\
  - Terminology\\
      1 Work-Items (CUDA-Threads) < Work-Groups < global NDRanges (Grid)\\
  - Basic information\\
      1 multiple partners (including NVIDIA)\\
      1 standardized programming model\\
      1 heterogeneous platform which needs explicit specification (Context)\\
        --> can also be used for FPGAs\\
        --> more complex than CUDA, but provides more diversity in HW\\
      


%---------------------------------------------------------------------------------

\section{Programming Optimization and Challenges}
  - increase Occupancy\\
  - increase latency\\
  - problems / concerns (warp divergence)\\


%---------------------------------------------------------------------------------

\section{Conclusion and Discussion}




\begin{enumerate}
\item \cite{Rauber.2012} (1)
\item \cite{Lindholm.2008} (2)
\item \cite{Burgess.2020} (3)
\item \cite{Huang.2008} (4)
\item \cite{Bialas.2016} (5)
\item \cite{Khronos.2019} (6)
\item \cite{Wang.2019} (7)
\item \cite{NVIDIA.2018} (8)
\item \cite{NVIDIA.2019} (9)
\item Intel Xeon 8280L
\end{enumerate}

Matrix-Matrix Multiplication
https://www.quantstart.com/articles/Matrix-Matrix-Multiplication-on-the-GPU-with-Nvidia-CUDA/

additional interisting sources:
  https://ieeexplore.ieee.org/abstract/document/4490127




% Literaturverzeichnis ------------------------------------------------
\newpage
\bibliographystyle{alphadinLinkLocal}
\bibliography{literatur}

%\iffalse
\end{document}
%\fi
