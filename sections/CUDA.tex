\section{CUDA - A GPU Programming Model}
\label{sec:CUDA}
  CUDA is a programming model developed by NVIDIA to enable an easier way to create programs using GPU acceleration.
  Its goal is to maintain a low learning curve for programmers and abstract the hardware design of GPUs.
  It is available for C, C++, Fortran, Python and MATLAB and provides keywords to explicitly specify parts to accelerate by the GPU in normal code.
  These parts are called \textit{kernel functions} or kernels and together are called \textit{device program}.
  All the parts running on the CPU are called \textit{host program} and all CUDA programs start with these.
  The host program can move data in the GPU memory and call a kernel function to utilize GPU acceleration.
  Device and host program then run in parallel until a synchronization or the end of the kernel function are reached.~\cite{NVIDIA.2019}~\cite{Rauber.2012}~\cite{Huang.2008}

  The declaration specifier to mark a function as host function is \texttt{\_\_host\_\_}.
  It is furthermore the default, in case no specifier is declaring a function.
  \texttt{\_\_global\_\_} can be used to declare kernel functions.
  These can be called from the host program and device program.
  Whereas \texttt{\_\_device\_\_} is used to declare kernel functions which are only callable by other kernel functions.
  When calling a kernel function the call has to contain an \textit{execution configuration} of structure \(<<<D_{g}, D_{b}>>>\).
  It specifies how many threads are used to execute the kernel function.
  Each of them executes the kernel function by its own.
  \(D_b\) tells the kernel how many threads build a block and \(D_g\) furthermore how many blocks form a grid.
  CUDA uses an own data type to do so --- \texttt{dim3}.
  This data type contains three integer values specifying the size of x, y and z dimension.
  However it is possible to use less dimensions by assigning the value one to a dimension.
  Inside a grid each block can be accessed by its block ID and a thread in a block can be accessed by thread IDs.
  These can be used to assign individual tasks to each thread.
  Each kernel call dynamically creates a new grid with the defined number of blocks and threads for that application.
  An example can be seen in listing \ref{lst:CUDA_EX}.%~\cite{Rauber.2012}~\cite{Huang.2008}

  To synchronize threads \texttt{\_\_syncthreads()} can be called.
  However, thread synchronization is only possible within a block.
  Therefore it necessary to design the program to execute each block independent from each other.%~\cite{Rauber.2012}~\cite{Huang.2008}

    
\subsection{Memory Hierarchy}
\label{subsec:MemHi}
  Another important aspect to consider when it comes to GPU programming is memory hierarchy.
  A GPU uses different types of memory with different accessibilities and sizes.
  Firstly, there are the global memory and the constant memory.
  They are shared across all blocks of a kernel function.
  However, both differ in accessibility.
  The host, which is the CPU making use of GPU acceleration, can access both via read and write operations to provide data to use.
  In contrast to that the GPU, also known as device, can only access the global memory by read and write.
  Constant memory can only be accessed by the device via read operations.
  Shared memory has a short access time compared to the previous mentioned memory types and is used by all threads of a block.
  Finally, the registers are local to each thread and have the shortest access time.
  Only the thread owning the registers can access them.~\cite{Rauber.2012}~\cite{Huang.2008}
     
  Figure \ref{fig:memorga} shows how the memory hierarchy looks like and how each level of memory can be accessed.
  The host can allocate device memory by \texttt{cudaMalloc()} and deallocate by \texttt{cudaFree()}.
  Data can the be moved by the host via \texttt{cudaMemcpy()}.
  The direction of the copy is specified by the last parameter of the function.
  An example for memory transfer can be seen in listing \ref{lst:CUDA_EX}.
  
  \bildbreite{figures/speicheroragnisation.jpg}{8cm}{fig:memorga}{GPU Memory Organization~\cite{Rauber.2012}}{}
  
      
\subsection{Hardware Mapping}
  CUDA scales the parallel application performance transparently to the computing architecture.
  This means that a compiled program can be executed on any GPU size without recompiling.
  The program does not care how many processors it uses.
  It is composed into independently defined blocks --- warps.
  The GPU work distribution unit generates a stream of thread blocks and distributes these blocks to available SMs.~\cite{Lindholm.2008}~\cite{NVIDIA.2019}
  
\subsection{Example Implementation}
  A CUDA example implementation of a matrix-matrix multiplication can be seen in listing \ref{lst:CUDA_EX}.
  It utilizes all the learnings of this chapter.
  The CUDA programming model uses the \textit{nvcc} compiler to generate an executable program.

\begin{lstlisting}[style=cpp,caption={Kernel Function of Matrix Multiplication},label={lst:CUDA_EX},numbers=left]
__global__ void SqMatrixMul(float* A, float* B, float* C, int N) {
    int ROW = blockIdx.y * blockDim.y + threadIdx.y;
    int COL = blockIdx.x * blockDim.x + threadIdx.x;
    float cell_sum = 0.0;

    if (ROW < N && COL < N) {
        for(int i = 0; i < N; i++){
            cell_sum += A[ROW * N + i] * B[i * N + COL];
        }
        C[ROW * N + COL] = cell_sum;
    }
}

int main(int argc, char* argv[]){
    // Allocate local memory
    float* host_A = (float*) malloc(size);
    float* host_B = (float*) malloc(size);
    float* host_C = (float*) malloc(size);
    // Allocate device memory
    float* device_A;
    cudaMalloc(&device_A, size);
    float* device_B;
    cudaMalloc(&device_B, size);
    float* device_C;
    cudaMalloc(&device_C, size);
    // Copy data from host to device
    cudaMemcpy(device_A, host_A, size, cudaMemcpyHostToDevice);
    cudaMemcpy(device_B, host_B, size, cudaMemcpyHostToDevice);
    // Call kernel function
    SqMatrixMul<<<blocksPerGrid,threadsPerBlock>>>(device_A, device_B,
		device_C, MATRIX_SIZE);      
    // Copy data from device to host
    cudaMemcpy(host_C, device_C, size, cudaMemcpyDeviceToHost);
    // Free device memory
    cudaFree(device_A);
    cudaFree(device_B);
    cudaFree(device_C);
}
\end{lstlisting}
