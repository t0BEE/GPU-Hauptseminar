\section{Conclusion and Discussion}
Having a growing attention and amount of data in the field of machine learning, general purpose GPUs will become more and more important for these fields.
Machine learning offers huge amounts of parallelizable data which will be processed by these accelerators.
This paper showed that GPUs may work faster than CPUs do --- even when utilizing multi-threading.
However, when programming with GPUs, the overhead of moving data has to be considered.
Furthermore, during the implementation of the matrix multiplication in CUDA and OpenCL, it came clear that CUDA is a better choice to introduce GPU programming.
The execution time in OpenCL and CUDA do not differ significantly however using CUDA is more convenient.
The lines of code needed to start a kernel function in CUDA are less.
This makes the programming less complex compared to OpenCL.
In OpenCL the programs context has to be defined and the parameters have to be passed one by one to the kernel function.
Therefore, the initialization time in OpenCL takes longer than in CUDA.
However, this is the case because OpenCL is not limited to GPU programming and can be used for FPGAs for example.
